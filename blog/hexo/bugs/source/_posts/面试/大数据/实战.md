一、阿里：

一面，主要问简历的项目，介绍负责的事情；写以下二叉树几种遍历的代码；写sql（实现行转列 ）；mysql语句是怎么优化的，索引如何优化，索引存储方式是什么数据结构 原理；redis支持什么类型原理及使用 ；介绍觉得最有成就感的项目；如何实现LRU；

二面， redis的原理的介绍 单线程为什么 单线程如何应对高并发请求；说一下oozie的原理或使用；笔试代码：写一个函数实现两个非常大的整数的加法。

二、百度

1、介绍storm项目，各个bolt；storm如何保证可靠性；storm 的grouping 策略；hbase读写数据过程；namenode作用 ；secondnamenode作用（不是介绍HA）；spark宽窄依赖；目前数据仓库如何分层；

2、sql：用hive和spark core实现 用户浏览表，车型枚举表 两个表关联，取pv的 top10;

3、写二分查找代码

三、某短视频公司

描述现在的数据处理架构；flume程序错误了怎么办？hadoop3.0什么改进？hdfs、hive、hbase等有做过什么优化；

spark excuter task 区别；client cluster区别 standalone是什么，map mappartiton区别，dataset dataframe区别；spark sql使用；spark程序如何切分stage；

描述垃圾回收机制；storm sparkstreaming区别，从原理设计上说明为什么spark是微批处理，不是单条处理。

讲述als,逻辑回归原理；itembased ，userbased原理；相似度距离有哪些？

写代码：

1、1-N个台阶，每次1或2，多少中上法。

2、找出链表的倒数第N个元素。

3、从坐标（0,0）出发，移动N步，最终落到那些点。

4、一个文件里，内容为 uid,date(如1,2018-12-11) 计算每日新用户量（实现方案多种，可以hive，可以spark core）。

四、其他公司

java 基本数据类型；java string内存怎么分配，new 两个一样的字符串 内存怎么分；解释spark shuffle；spark on yarn ，spark程序执行过程 (client cluster区别）；map，mappartition区别；目前的数据仓库如何搭建，讲述java多线程的了解；

1、RDD中reduceBykey与groupByKey哪个性能好，为什么

reduceByKey：reduceByKey会在结果发送至reducer之前会对每个mapper在本地进行merge，有点类似于在MapReduce中的combiner。这样做的好处在于，在map端进行一次reduce之后，数据量会大幅度减小，从而减小传输，保证reduce端能够更快的进行结果计算。

groupByKey：groupByKey会对每一个RDD中的value值进行聚合形成一个序列(Iterator)，此操作发生在reduce端，所以势必会将所有的数据通过网络进行传输，造成不必要的浪费。同时如果数据量十分大，可能还会造成OutOfMemoryError。

通过以上对比可以发现在进行大量数据的reduce操作时候建议使用reduceByKey。不仅可以提高速度，还是可以防止使用groupByKey造成的内存溢出问题。

2、讲述一下hdfs上传文件的流程。

答：这里描述的 是一个256M的文件上传过程

① 由客户端 向 NameNode节点节点 发出请求；

②NameNode 向Client返回可以可以存数据的 DataNode 这里遵循机架感应原则；

③客户端 首先 根据返回的信息 先将 文件分块（Hadoop2.X版本 每一个block为 128M 而之前的版本为 64M；

④然后通过那么Node返回的DataNode信息 直接发送给DataNode 并且是 流式写入同时会复制到其他两台机器；

⑤dataNode 向 Client通信 表示已经传完 数据块 同时向NameNode报告 ⑥依照上面（④到⑤）的原理将 所有的数据块都上传结束 向 NameNode 报告 表明 已经传完所有的数据块 。

3、了解zookeeper吗？介绍一下它，它的选举机制和集群的搭建。

答：那当然是熟悉啦，ZooKeeper 是一个开源的分布式协调服务，是 Google Chubby 的开源实现。分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。我们公司使用的flume集群，Kafka集群等等，都离不开ZooKeeper呀。每个节点上我们都要搭建ZooKeeper服务。首先我们要在每台pc上配置zookeeper环境变量，在cd到zookeeper下的conf文件夹下在zoo_simjle.cfg文件中添加datadir路径，再到zookeeper下新建data文件夹，创建myid，在文件里添加上server的ip地址。在启动zkserver.sh start便ok了。

4、spark streming在实时处理时会发生什么故障，如何停止，解决

答：和Kafka整合时消息无序：

修改Kafka的ack参数，当ack=1时，master确认收到消息就算投递成功。ack=0时，不需要收到消息便算成功，高效不准确。sck=all，master和server都要受到消息才算成功，准确不高效。

StreamingContext.stop会把关联的SparkContext对象也停止，如果不想把SparkContext对象也停止的话可以把StremingContext.stop的可选参数stopSparkContext设为flase。一个SparkContext对象可以和多个streamingcontext对象关联。只要对前一个stremingcontext.stop(stopsparkcontext=false),然后再创建新的stremingcontext对象就可以了。

5、说一下你对yarn的理解：

答：YARN是Hadoop2.0版本引进的资源管理系统，直接从MR1演化而来。

核心思想：将MR1中的JobTracker的资源管理和作业调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现。

ResourceManager：负责整个集群的资源管理和调度 ；ApplicationMaster：负责应用程序相关事务，比如任务调度、任务监控和容错等。

YARN的出现，使得多个计算框架可以运行在同一个集群之中。 1. 每一个应用程序对应一个ApplicationMaster。 2. 目前可以支持多种计算框架运行在YARN上面，比如MapReduce、storm、Spark、Flink。

6、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词。

答：Step1：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为f0 ,f1 ,... ,f4999）中，这样每个文件大概是200k左右，如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M；

Step2：对每个小文件，统计每个文件中出现的词以及相应的频率（可以采用trie树/hash_map等），并取出出现频率最大的100个词（可以用含100个结点的最小堆），并把100词及相应的频率存入文件，这样又得到了5000个文件；

Step3：把这5000个文件进行归并（类似与归并排序）；

7、如何配置spark master的HA？

答：1)配置zookeeper

2)修改spark_env.sh文件,spark的master参数不在指定，添加如下代码到各个master节点

ExportSPARK_DAEMON_JAVA_OPTS=-Dspark.deploy.recoveryMode=ZOOKEEPER-

Dspark.deploy.zookeeper.url=zk01:2181,zk02:2181,zk03:2181-Dspark.deploy.zookeeper.dir=/spark”

3) 将spark_env.sh分发到各个节点

4)找到一个master节点，执行./start-all.sh，会在这里启动主master,其他的master备节点，启动master命令: ./sbin/start-master.sh

5)提交程序的时候指定master的时候要指定三台master，例如

./spark-shell –master spark://master01:7077,master02:7077,master03:7077

8、一个datanode 宕机,怎么一个流程恢复

答：Datanode宕机了后，如果是短暂的宕机，可以实现写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经被备份到其他机器了，那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动。


阿里面试：

一面过程

首先是一次不记名面试，这里我也真的很感谢这场不记名面试了，如果没有这场不记名面试，我可能真的一面就结束了。因为我之前是没有任何找工作的面试经验的，再加上去阿里对我来说真的是一件几个月做梦都会梦到的事情，我把它看的太重了，所以在面试之前我经历人生中最紧张最紧张的一段时间，导致这次面试基本是全面GG。我先来说一下这次面试的过程吧。

面试我的是一位小姐姐(这是我之前没想到的，因为我觉得肯定是个男生)，我先是自我介绍，然后我感觉我介绍的很着急，而且没有逻辑性(充分暴露了我的准备不足)，介绍完就感觉很不好，此时已经感觉有点凉了。

然后就问了第一个问题：springcloud的三个原则是什么?

我不知道啊，springcloud我才学了一天啊，只会用啊，所以第一个问题就GG了，我更紧张了。之后小姐姐问我，你对哪块有比较深入的研究，我因为之前对redis看了不少，所以我就说redis还仔细研究过一点，然后就又问了一个问题，那个问题中关于redis的名词我都没听过，这个时候我已经完全慌了，两个问题都没答出来。后来又聊着聊着说到了多线程;

第三个问题：一个线程销毁之后还能再start嘛?

我：能吧?(我没有注意过啊)所以相当于问了三个问题都没答出来，小姐姐似乎感觉到我的水平了，所以就没再多问了，就讲起了我这几个月的生活，早起看算法，白天听课，晚上调电路还得复习白天的，每天已经用尽了我最大的力气了，我也说到了自己面的不好，(事实上第一是因为我很紧张，第二我确实有的地方掌握的不是很深入)。

她很亲切的说没关系，几个月能成这样已经不错了。然后最后聊了聊别的就结束了。其实后来我才知道这个小姐姐是清华的，更加坚定了我去阿里的决心，我渴望去一个周围都是大神的地方磨炼自己!

结束之后我仔细分析今天的面试，总结了一些经验，而且深刻的明白是要稍微准备一下，尤其是并发编程这块的知识点，所以接下来的几天我疯狂弥补之前的一些知识漏洞，等待接下来的正式面试。

几天后是一面，一面整体的节奏比较缓，主要是以技术问题为主，没有加入太多的聊天内容，面的也比较久，一共面了一个多小时，下面是一面的题目：

1、线程实现的方式

2、线程池的理解

3、hashmap的底层，你怎么了解的

4、jvm底层

5、垃圾回收机制

6、怎么理解mvc

7、排序算法选一种(我选的快排)

8、单点登录引出来的一个问题，如果通过A域名访问了A系统，通过B域名访问了B系统，B系统如何知道此客户端也访问了A系统

9、类加载机制

10、说一说ThreadLocal的理解

11、爬虫用什么做的，怎么做的

12、如果爬取到的数据格式不一致怎么办(针对我的那个新闻网站)

13、你对你这个项目的数据有没有一些其他想法?如果让你重新做，你会有哪些改善

14、你这个项目的架构说一说

整体来说，一面的基础性问题比较多，所以博主基本都答出来了，准备过还是有些用处的。所以一面完了感觉还不错，接着第二天下午就二面了。

二面过程

二面是我最慌的面试，因为二面是最关键的，是最大的主管面试，所以非常关键，整体来说，二面的节奏非常快，和一面完全不同，面试官很希望我言简意赅的说完答案，所以面的不算很好，当时完了又以为GG了，好在最后还是幸运的过了，面试问题如下：

1、静态方法和实例方法的同步有什么不同?

2、future类的作用

3、红黑树比平衡二叉树的优势

4、知道最大堆，最小堆嘛?描述一下堆排序

5、redis是单线程还是多线程的，说说redis的优势

6、了解AOP嘛，基于什么原理的

7、3点15分的夹角是多少度

二面面试官问的很急，我答的也快，所以有两个问题答的不好，但是面试官让我觉得有一种肃然起敬的感觉，因为他说话比较快，但是都能说到点子上，最关键的说的话非常让人容易接受。

二面过了，当天晚上我就接到了三面的电话(阿里的效率真的让我瞠目结舌)

三面过程

三面是交叉面试了，基本都围绕我的项目展开，我经过前面几次的面试之后，这个时候已经拥有了很高的面试经验值了，所以三面的时候我一点不慌了，回答问题非常稳重(一面刚开始的时候腿在抖。。。)，三面的问题如下：

1、介绍以下你项目的特色

2、说说你这个项目的分工，以及在团队协作之间遇到的问题

3、用java爬虫有什么不好

4、谈谈你对mycat的理解以及如何进行数据库的双机热备

5、谈谈redis如何构建集群，数据在底层是如何进行同步的

6、网络连接和断开的过程?为什么是4次挥手不是3次?

7、数据库事务的四个特性

8、volatile关键字的作用

9、估算一下7的128次多少位(这个数学问题，我当时没想出思路，只估算了个100，虽然蒙对了，但是思路没给出，事实上这个题的思路有点难的)

三面还算比较顺利，基本都是我在说，所以也很快过了，第二天早上就收到了HR终面，两天4面，鬼知道我那两天经历了什么。。。

HR就基本问一些基本情况了，没有技术问题，只要人不是不正常感觉都没啥问题。

经验总结

总结一下面试经验吧，虽然我只经历了阿里的面试，但是这几场面试足以让我总结出面试的门道了：

1、自我介绍非常重要，基本上是主导你的整个面试的过程的东西了，一定要好好准备自我介绍，把面试官往你熟悉的地方引

2、见机行事，如果面试官不急，你就不要急，回到的时候多说一点，尽量把只是穿起来，往你熟悉的地方引，比如博主的1面，问我HashMap的底层，我就不仅说了它底层是一个数组加链式栈，还说了链式栈的目的是解决哈希冲突，还说了解决哈希冲突有两种方法：一种拉链法，一种线性探测法，hashmap采用了前者，(我现在还知道了ThreadLocalMap采用后者，这个知道的人不多，如果你顺着引出来，加分很多)。如果面试官比较急，证明他有很多人面，他希望在最短的时间里让你说出关键的东西，比如博主的二面，我们就不需要扩展说，言简意赅即可

3、一定要好好准备，对一些常用的内容进行深入的探究，比如现在面试必问的多线程高并发，JVM底层，数据结构和算法，一定要 挑一些进行深入的探究

4、最好有一个完整的项目或者有一个担任重要角色的项目，对这个项目非常熟悉，并且要想好项目开发过程中遇到的问题以及如何解决的

5、不装，会就是会，不会就是不会，不要猜，我二面就犯了错，面试官问我redis单线程还是多线程，我随口一说：多线程吧?面试官：猜的吧?我：是。。。。面试官：不会不要猜!所以我当时慌的一批!
